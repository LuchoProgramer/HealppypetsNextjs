# llms.txt - Lista de modelos/instancias LLM usadas por el proyecto
# Ubicación sugerida: raíz del repo (puedes moverlo a /config o /src/config si prefieres).
# Formato recomendado (línea por entrada, campos separados por | - pipe):
# name | provider | endpoint | api_key_env | model | timeout_seconds | notes
# - name: identificador corto (sin espacios)
# - provider: p.ej. openai, anthropic, local, llama.cpp
# - endpoint: URL base o ruta local (para servidores locales)
# - api_key_env: nombre de la variable de entorno que contiene la API key (opcional)
# - model: nombre del modelo (p.e., gpt-4o-mini) (opcional)
# - timeout_seconds: entero (opcional)
# - notes: comentario libre (opcional)
#
# Ejemplos:
openai_main|openai|https://api.openai.com/v1|OPENAI_API_KEY|gpt-4o-mini|30|Producción - usado para chat y completions
local_llama|local|http://127.0.0.1:8080||llama-2-13b||Servidor local para pruebas
anthropic_chat|anthropic|https://api.anthropic.com/v1|ANTHROPIC_API_KEY|claude-instant|20|Fallback rápido

# Si prefieres JSON, puedo generar un archivo llms.json con un array de objetos.
# Indica si quieres que lo cree en lugar de este archivo de texto.
